# app/ai/chains/instruction_chain.py
import logging
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from app.ai.llm_manager import llm_manager
from app.ai.multi_llm_manager import multi_llm_manager
from app.ai.prompts.hebrew_prompts import (
    HEBREW_SYSTEM_PROMPT,
    HEBREW_BREAKDOWN_PROMPT,
    HEBREW_EXAMPLE_PROMPT,
    HEBREW_EXPLAIN_PROMPT,
    HEBREW_PRACTICE_PROMPT
)
from app.ai.prompts.base_prompts import (
    INSTRUCTION_ANALYSIS_PROMPT,
    PRACTICE_BREAKDOWN_PROMPT,
    PRACTICE_EXAMPLE_PROMPT,
    PRACTICE_EXPLAIN_PROMPT
)

logger = logging.getLogger(__name__)

class InstructionProcessor:
    def __init__(self):
        self.llm = llm_manager.get_llm()  # Fallback LLM
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
    def _get_llm_for_provider(self, provider: str = None):
        """Get LLM instance for specified provider"""
        if provider and provider in multi_llm_manager.providers:
            provider_instance = multi_llm_manager.providers[provider]
            # Handle different provider types
            if hasattr(provider_instance, 'llm'):
                return provider_instance.llm
            elif hasattr(provider_instance, 'client'):
                # For Google and other providers that use 'client' instead of 'llm'
                return provider_instance
            else:
                return None
        return self.llm  # Fallback to default
    
    def _get_prompts_for_language(self, language_preference: str = 'he'):
        """
        Dynamically select prompts based on user language preference
        Default: Hebrew ('he') for Israeli educational system
        Only use English for explicitly international users
        """
        # Use English prompts ONLY if explicitly requested
        if language_preference and language_preference.lower() in ['en', 'english']:
            return {
                'practice': INSTRUCTION_ANALYSIS_PROMPT,
                'breakdown': PRACTICE_BREAKDOWN_PROMPT,
                'example': PRACTICE_EXAMPLE_PROMPT,
                'explain': PRACTICE_EXPLAIN_PROMPT,
                'analysis': INSTRUCTION_ANALYSIS_PROMPT
            }
        else:
            # Default to Hebrew (Israeli educational system)
            # Covers: 'he', 'hebrew', None, 'en' (changed to default Hebrew)
            return {
                'practice': HEBREW_PRACTICE_PROMPT,
                'breakdown': HEBREW_BREAKDOWN_PROMPT,
                'example': HEBREW_EXAMPLE_PROMPT,
                'explain': HEBREW_EXPLAIN_PROMPT,
                'analysis': HEBREW_PRACTICE_PROMPT
            }
    
    def analyze_instruction(self, instruction: str, student_context: dict, provider: str = None) -> dict:
        """Analyze an instruction to understand what needs to be done"""
        
        # For cloud models, use efficient prompt with system guidance
        if provider and not provider.startswith("ollama-"):
            # Get conversation history
            conversation_history = student_context.get("conversation_history", "")

            # Check for typos/gibberish that might mean Hebrew assistance words
            instruction_clean = instruction.lower()
            # Common typos for Hebrew assistance words
            if any(typo in instruction_clean for typo in ['xchr', 'xsbir', 'hsbr', 'explain']):
                instruction_interpretation = "住专"
            elif any(typo in instruction_clean for typo in ['breakdown', 'steps', 'pirok']):
                instruction_interpretation = "驻专拽 砖"
            elif any(typo in instruction_clean for typo in ['example', 'dugma', '']):
                instruction_interpretation = ""
            else:
                instruction_interpretation = instruction

            # Short, efficient prompt - guide student to choose assistance type
            prompt_text = f"""转 专 (LearnoBot), 注专 AI 砖注专 转 注 拽转 . 转注 转 专 砖专转 转,  转住专   注砖转.

拽 砖:
-  转转 转砖转 砖专转  驻转专转 
-  转爪 注 砖 拽 拽住 砖转 住驻拽
- 专拽  注专 ,  转驻转专 拽 转
-  转专 注 拽住 砖转 专 砖

住专转 砖:
{conversation_history}

转 砖 注砖: "{instruction_interpretation}"

 转 砖 砖 注 拽住  专  专 住驻拽 拽住:
转: " 爪专 专转 转 拽住  注专 . 驻砖专 砖 转  拽 转 拽住?"

 转 专 住驻拽 拽住 住专 拽砖 注专:
-  拽砖 "住专": 住专 转 拽住/砖 驻砖转
-  拽砖 "驻专拽 砖": 驻专拽 转 砖 爪注
-  拽砖 "": 转  专转

专转,   注专 砖砖 专:
 **住专** - 住专   专
 **驻专拽 砖** - 拽 砖转 拽转
 **** - 转转  

 转专爪 砖注专 ?"""
        else:
            # Use existing complex prompts for local models
            language_pref = student_context.get("language_preference", "he")
            prompts = self._get_prompts_for_language(language_pref)
            
            # Format the prompt with variables
            if language_pref and language_pref.lower() in ['en', 'english']:
                prompt_text = prompts['analysis'].format(
                    instruction=instruction,
                    student_context=str(student_context)
                )
            else:
                prompt_text = prompts['analysis'].format(
                    instruction=instruction,
                    student_level=student_context.get("difficulty_level", 3),
                    assistance_type="住专"
                )
        
        # Use multi_llm_manager to generate response
        result = multi_llm_manager.generate(prompt_text, provider=provider)
        
        return {"analysis": result}
    
    def breakdown_instruction(self, instruction: str, student_level: int, language_preference: str = "he", provider: str = None) -> str:
        """Break down instruction into simple steps"""
        
        # For cloud models, use efficient short prompt
        if provider and not provider.startswith("ollama-"):
            from app.ai.prompts.hebrew_prompts import HEBREW_BREAKDOWN_SHORT
            prompt_text = HEBREW_BREAKDOWN_SHORT.format(instruction=instruction)
        else:
            # Use existing prompts for local models
            prompts = self._get_prompts_for_language(language_preference)
            prompt_text = prompts['breakdown'].format(
                instruction=instruction,
                student_level=student_level
            )
        
        return multi_llm_manager.generate(prompt_text, provider=provider)
    
    def provide_example(self, instruction: str, concept: str, language_preference: str = "he", provider: str = None) -> str:
        """Provide a relatable example"""
        
        # For cloud models, use efficient short prompt
        if provider and not provider.startswith("ollama-"):
            from app.ai.prompts.hebrew_prompts import HEBREW_EXAMPLE_SHORT
            prompt_text = HEBREW_EXAMPLE_SHORT.format(instruction=instruction)
        else:
            # Use existing prompts for local models
            prompts = self._get_prompts_for_language(language_preference)
            prompt_text = prompts['example'].format(
                instruction=instruction,
                concept=concept
            )
        
        return multi_llm_manager.generate(prompt_text, provider=provider)
    
    def explain_instruction(self, instruction: str, student_level: int, language_preference: str = "he", provider: str = None) -> str:
        """Explain instruction in simple terms"""
        
        # For cloud models, use efficient short prompt
        if provider and not provider.startswith("ollama-"):
            from app.ai.prompts.hebrew_prompts import HEBREW_EXPLAIN_SHORT
            prompt_text = HEBREW_EXPLAIN_SHORT.format(instruction=instruction)
        else:
            # Use existing prompts for local models
            prompts = self._get_prompts_for_language(language_preference)
            prompt_text = prompts['explain'].format(
                instruction=instruction,
                student_level=student_level
            )
        
        return multi_llm_manager.generate(prompt_text, provider=provider)
